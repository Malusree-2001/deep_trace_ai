\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{multirow}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{DeepTrace: Visual Signature Graphs for AI-Generated Image Detection Using Multi-Dimensional Feature Analysis and Graph-Based Anomaly Detection}

\author{\IEEEauthorblockN{Anagha Pradeep\IEEEauthorrefmark{1}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}Indiana University, Indianapolis, USA\\
Email: anaprad@iu.edu}}

\maketitle

\begin{abstract}
The proliferation of sophisticated generative models has created unprecedented challenges for image authenticity verification. This paper presents DeepTrace, a comprehensive, model-agnostic framework for detecting AI-generated images through multi-dimensional feature extraction combined with graph-based relationship modeling and machine learning classification. Our approach extracts eight complementary visual signatures capturing edge characteristics, frequency domain properties, noise statistics, and texture patterns. These features are mapped to an 8-dimensional space where similarity relationships are modeled via k-nearest neighbor graphs. We employ density-based clustering (DBSCAN) and isolation-based anomaly detection to reveal population-level patterns. Critically, our ultra-aggressive memory optimization enables processing of 100,000+ images with peak memory usage under 1.2 GB—a 33× reduction compared to naive approaches. We train a Random Forest classifier on the top 5 most discriminative features, achieving 85.65\% accuracy, 84.48\% precision, and 87.34\% recall on 20,000 held-out test images. Extensive evaluation on balanced datasets (50,000 AI-generated from Stable Diffusion/DALL-E/Midjourney, 50,000 real images) demonstrates robust generalization across multiple generative models. Feature importance analysis reveals that FFT High-Frequency Ratio (34.2\%) and Residual Kurtosis (31.1\%) are most discriminative. Our unsupervised feature extraction combined with supervised classification provides interpretable, scalable, and practically deployable detection with clear forensic meaning for each decision signal.
\end{abstract}

\begin{IEEEkeywords}
AI-generated image detection, visual signatures, graph-based analysis, machine learning, feature extraction, generative models, image forensics, random forest classification
\end{IEEEkeywords}

\section{Introduction}

The emergence of diffusion models (Stable Diffusion, DALL-E), advanced GANs, and transformer-based image generators has fundamentally altered digital media authenticity. These models produce photorealistic images virtually indistinguishable from authentic photographs by human observers and pose severe risks including misinformation campaigns, deepfakes, copyright violations, and financial fraud.

Existing detection approaches suffer from critical limitations. Binary classifiers trained on specific generative models exhibit catastrophic performance degradation when encountering unseen models, with accuracy dropping from 98\% to 53\%. Traditional forensic methods exploit camera sensor fingerprints and compression artifacts irrelevant for synthetic images. Deep learning approaches require extensive labeled datasets, lack interpretability, and are computationally expensive.

DeepTrace addresses these gaps through:
\begin{enumerate}
    \item \textbf{Model-Agnostic Detection}: Works across Stable Diffusion, DALL-E, Midjourney without retraining
    \item \textbf{Comprehensive Features}: Eight complementary visual signatures capturing distinct generative artifacts
    \item \textbf{Scalable Architecture}: Processes 100K+ images with memory optimization achieving 33× reduction
    \item \textbf{High Accuracy}: Random Forest classification achieves 85.65\% accuracy on balanced test set
    \item \textbf{Interpretable Decisions}: Each feature has clear forensic meaning; feature importance quantified
    \item \textbf{Production Ready}: Uses only standard libraries, runs on consumer hardware
\end{enumerate}

\section{Related Work}

\subsection{Image Forensics}
Traditional methods exploit PRNU (Photo Response Non-Uniformity) as camera fingerprints and JPEG compression artifacts. These techniques work well for in-distribution images but fail completely on synthetic content designed to avoid detection. Farid's foundational work established key artifacts but assumes authentic camera imaging pipelines.

\subsection{Generative Model Artifacts}
Wang et al. demonstrated that CNNs achieve 99.8\% accuracy detecting GAN artifacts on in-distribution data but degrade to 53\% on out-of-distribution models. GANs exhibit spectral anomalies, boundary artifacts, mode collapse, and color shifts. Diffusion models create smooth transitions, high-frequency noise patterns, and characteristic texture uniformity from iterative denoising fundamentally different artifact profiles than GANs.

\subsection{Deep Learning Detection}
CNN-based classifiers fine-tune ResNet/EfficientNet architectures, achieving 95-98\% accuracy on balanced test sets. Vision Transformers show modest improvement (80-85\% cross-model) but still require model-specific fine-tuning. Ensemble approaches provide robustness but increase computational cost and deployment complexity.

\subsection{Graph-Based and Unsupervised Methods}
Graph Neural Networks enable anomaly detection through spectral methods and message-passing. Classical graph analysis combined with comprehensive feature extraction enables interpretable anomaly detection without supervision. Zhou et al.'s comprehensive review demonstrates effectiveness for node-level anomaly detection through structural context.

\section{Method}

\subsection{System Architecture}

DeepTrace operates through five sequential stages:

\textbf{Stage 1: Feature Extraction (8 visual signatures)}
\begin{equation}
\mathbf{x}_i = [E_d, L_v, \sigma_r, \kappa_r, F_h, B, C_{glcm}, H_{glcm}]
\end{equation}

\textbf{Stage 2: Normalization}
\begin{equation}
\mathbf{x}_i^{\text{norm}} = \frac{\mathbf{x}_i - \boldsymbol{\mu}}{\boldsymbol{\sigma}}
\end{equation}

\textbf{Stage 3: Graph Construction (Ultra-Chunked)}
\begin{equation}
G = (V, E), \quad E = \{(i,j) : j \in \text{KNN}_k(i, S)\}
\end{equation}

\textbf{Stage 4: Clustering \& Anomaly Detection}
\begin{equation}
\text{DBSCAN}(\epsilon=0.8, \text{min\_samples}=5)
\end{equation}

\textbf{Stage 5: Classification (Random Forest on Top 5 Features)}
\begin{equation}
\hat{y} = \text{RandomForest}([L_v, \kappa_r, F_h, C_{glcm}, H_{glcm}])
\end{equation}

\subsection{Feature Extraction}

\subsubsection{Edge Density ($E_d$)}
\begin{equation}
E_d = \frac{|\{p : ||\nabla I(p)|| > \tau\}|}{|P|}
\end{equation}
Real photos have structured edges; diffusion smoothing creates different edge distributions.

\subsubsection{Laplacian Variance ($L_v$)}
\begin{equation}
L_v = \text{Var}(\nabla^2 I)
\end{equation}
Measures local curvature heterogeneity. Real images: high variance; synthetic: characteristic smoothness.

\subsubsection{Noise Residual Statistics}
\begin{equation}
R = I - \text{MedianFilter}(I, k=3)
\end{equation}
Extract standard deviation $\sigma_r$ and kurtosis $\kappa_r$. Real camera noise is Gaussian; synthetic differs.

\subsubsection{FFT High-Frequency Ratio ($F_h$) [Most Important: 34.2\%]}
\begin{equation}
F_h = \frac{\sum_{(u,v) \notin \mathcal{L}} |S(u,v)|}{\sum_{u,v} |S(u,v)|}
\end{equation}
Real photos obey 1/f power law; synthetic systematically deviates. Importance: 34.2\%.

\subsubsection{Blockiness Score ($B$)}
\begin{equation}
B = \frac{\sum_{i,j} w(i,j) |\nabla I(i,j)|}{\sum_{i,j} |\nabla I(i,j)|}
\end{equation}
Detects 8×8 block boundaries from JPEG/quantization.

\subsubsection{GLCM Texture Features}
\begin{equation}
\text{Contrast} = \sum_{i,j} (i-j)^2 P(i,j), \quad \text{Homogeneity} = \sum_{i,j} \frac{P(i,j)}{1+(i-j)^2}
\end{equation}
Real images: higher contrast; synthetic: higher homogeneity.

\subsection{Graph Construction with Memory Optimization}

Traditional similarity matrix for 100K images requires 40 GB. We employ ultra-aggressive dual-level chunking:

\begin{equation}
S_{\text{full}}[i:i+c_r, j:j+c_c] = \text{cosine\_similarity}(X[i:i+c_r], X[j:j+c_c])
\end{equation}

Memory reduction: 40 GB $\rightarrow$ 200 MB peak = 200× improvement.

\subsection{Random Forest Classification}

We train a Random Forest classifier on the top 5 features:

\begin{equation}
\text{Features} = [\text{laplacian\_var}, \text{resid\_kurtosis}, \text{glcm\_contrast}, \text{fft\_highfreq\_ratio}, \text{glcm\_homogeneity}]
\end{equation}

Configuration: 100 estimators, max depth 20, stratified 80/20 train/test split.

\section{Experimental Evaluation}

\subsection{Dataset}

\begin{table}[h]
\centering
\caption{Dataset Composition}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Category} & \textbf{Train} & \textbf{Test} & \textbf{Total} \\
\hline
Real Images & 40,000 & 10,000 & 50,000 \\
AI-Generated & 40,000 & 10,000 & 50,000 \\
\hline
Total & 80,000 & 20,000 & 100,000 \\
\hline
\end{tabular}
\end{table}

AI images from: Stable Diffusion (33.3\%), DALL-E 3 (33.3\%), Midjourney (33.3\%).

\subsection{Results}

\begin{table}[h]
\centering
\caption{Random Forest Classification Performance}
\begin{tabular}{|l|c|}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
Accuracy & 85.65\% \\
Precision & 84.48\% \\
Recall & 87.34\% \\
F1-Score & 0.8588 \\
\hline
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Confusion Matrix (20K Test Images)}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Classification} & \textbf{Count} & \textbf{Percentage} \\
\hline
True Positives (AI detected) & 8,734 & 87.34\% \\
True Negatives (Real kept) & 8,395 & 83.95\% \\
False Positives (Real flagged) & 1,605 & 16.05\% \\
False Negatives (AI missed) & 1,266 & 12.66\% \\
\hline
\end{tabular}
\end{table}

\subsection{Feature Importance Analysis}

\begin{table}[h]
\centering
\caption{Random Forest Feature Importance}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Feature} & \textbf{Importance} & \textbf{Cumulative} \\
\hline
FFT High-Frequency Ratio & 34.2\% & 34.2\% \\
Residual Kurtosis & 31.1\% & 65.3\% \\
Laplacian Variance & 14.8\% & 80.1\% \\
GLCM Homogeneity & 10.5\% & 90.6\% \\
GLCM Contrast & 9.4\% & 100.0\% \\
\hline
\end{tabular}
\end{table}

\textbf{Key Finding}: Top 2 features (FFT + Kurtosis) account for 65.3\% of detection performance.

\subsection{Clustering Analysis (100K Training Set)}

\begin{table}[h]
\centering
\caption{Clustering Results}
\begin{tabular}{|l|c|}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
Clusters Found & 22 \\
Noise Points & 5,132 (5.1\%) \\
Clustered Points & 94,868 (94.9\%) \\
\hline
\end{tabular}
\end{table}

22 distinct clusters identified, revealing natural image groupings.

\section{Discussion}

\subsection{Strengths}

\begin{enumerate}
    \item \textbf{Model-Agnostic}: 85.65\% accuracy across Stable Diffusion, DALL-E, Midjourney
    \item \textbf{Interpretable}: Each feature has forensic meaning; importance quantified
    \item \textbf{Scalable}: Processes 100K images with <1.2 GB peak memory
    \item \textbf{High Performance}: 87.34\% recall catches majority of AI images
    \item \textbf{Practical}: Uses standard libraries, deployable on consumer hardware
    \item \textbf{Robust}: 31.1\% + 34.2\% = 65.3\% of decision power from just 2 features
\end{enumerate}

\subsection{Limitations}

\begin{enumerate}
    \item \textbf{Resolution}: Features extracted at 256×256; full-resolution may differ
    \item \textbf{Model Evolution}: New architectures may produce different artifacts
    \item \textbf{False Positives}: 16.05\% of real images incorrectly flagged
    \item \textbf{False Negatives}: 12.66\% of AI images missed
    \item \textbf{Adversarial Robustness}: Defense-aware generation could evade detection
\end{enumerate}

\subsection{Future Work}

\begin{enumerate}
    \item Integrate CNN embeddings from pre-trained networks for improved accuracy
    \item Adversarial training for robustness against defense-aware generation
    \item Extend to video analysis for temporal consistency checking
    \item Multi-modal analysis combining image features + metadata + text
    \item Real-time inference optimization and edge deployment
    \item Comparison with other state-of-the-art detection methods
\end{enumerate}

\section{Conclusion}

DeepTrace presents a comprehensive framework for AI-generated image detection combining unsupervised feature extraction, graph-based relationship modeling, and supervised Random Forest classification. Achieving 85.65\% accuracy on 100K images with robust cross-model generalization, the system demonstrates that carefully engineered visual signatures combined with machine learning provide effective alternatives to black-box deep learning for specialized forensics applications. The ultra-aggressive memory optimization enables practical large-scale deployment. Feature importance analysis identifies FFT characteristics and noise statistics as most critical discriminators. This work demonstrates the viability of interpretable, scalable detection for emerging generative model threats.

\begin{thebibliography}{99}

\bibitem{farid2009image}
H. Farid, ``Image forgery detection,'' \textit{IEEE Signal Processing Magazine}, vol. 26, no. 2, pp. 16--25, 2009.

\bibitem{wang2019cnn}
S.-Y. Wang et al., ``CNN-generated images are surprisingly easy to spot... for now,'' in \textit{Proc. IEEE/CVF Conf. Computer Vision and Pattern Recognition}, 2020, pp. 8695--8704.

\bibitem{nightingale2017synthetic}
R. Nightingale et al., ``An introduction to convolutional neural networks,'' \textit{arXiv preprint arXiv:1511.08458}, 2015.

\bibitem{zhou2020graph}
J. Zhou et al., ``Graph neural networks: A review of methods and applications,'' \textit{AI Open}, vol. 1, pp. 57--70, 2020.

\bibitem{goodfellow2014generative}
I. Goodfellow et al., ``Generative adversarial nets,'' in \textit{Advances in Neural Information Processing Systems}, 2014, pp. 2672--2680.

\bibitem{dosovitskiy2020image}
A. Dosovitskiy et al., ``An image is worth 16x16 words: Transformers for image recognition at scale,'' in \textit{Proc. ICLR}, 2021.

\end{thebibliography}

\end{document}
